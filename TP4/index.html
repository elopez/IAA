<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Trabajo Práctico 4: k-vecinos</title>
<link rel="stylesheet" href="https://stackedit.io/res-min/themes/base.css" />
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body><div class="container"><h1 id="trabajo-práctico-4-k-vecinos">Trabajo Práctico 4: k-vecinos</h1>



<h2 id="ejercicios-a-y-d">Ejercicios A y D</h2>

<p>Se implementó un programa para resolver problemas de clasificación mediante el algoritmo de los k vecinos más cercanos (<em>k-NN</em>). El programa está realizado en C++ y puede ser compilado de dos formas distintas, dependiendo de la funcionalidad deseada por el usuario:</p>

<ul>
<li>Para clasificar observando los <code>k</code> vecinos más cercanos: <br>
<code>g++ -O2 knn.cpp -o knn.k</code></li>
<li>Para clasificar observando todos los vecinos a una distancia cuadrada <code>d</code> <br>
<code>g++ -O2 -DUSE_DIST knn.cpp -o knn.dist</code></li>
</ul>

<p>En el primer caso, para clasificar un punto el programa busca los <code>k</code> vecinos más cercanos al punto dado según la métrica euclidiana y utiliza la clase con más representantes en el conjunto. </p>

<p>En el segundo caso, para clasificar un punto se buscan todos los vecinos a una distancia no mayor a <code>d</code> y se utiliza la clase con más representantes en el conjunto. En caso de resultar vacío el conjunto, se utiliza la clase del vecino más cercano (similar al primer caso con <code>k = 1</code>)</p>

<p>En ambos casos, para seleccionar el valor de <code>k</code> o <code>d</code>  mediante validación, se particiona equitativamente el conjunto de entrenamiento en 5, y se utiliza la mediana del error de validación en dichos conjuntos.</p>

<h1 id="ejercicio-b">Ejercicio B</h1>

<p>Se procedió a clasificar nuevamente el <em>dataset</em> de espirales anidadas anteriormente utilizado con redes neuronales. En esta ocasión se utilizó el algoritmo del vecino más cercano (<code>k = 1</code>), obteniéndose un error de entrenamiento de 5.56% y un error de prueba de 5.8%. Este resultado es mejor que el obtenido con redes neuronales, siendo que aquellas resultaban en un error de prueba del orden de 9% con 40 neuronas en la capa oculta.</p>

<p>Al <a href="out-ejb/points-plot.pdf">graficar el conjunto de predicciones</a> se observa en general una muy buena clasificación de los puntos. Los errores se ubican sobre el área lindera entre ambos espirales. Esto se debe a que en esa zona es posible que el vecino más cercano a un punto de una espiral pertenezca a la otra, cosa que nunca ocurre en la zona interna de la espiral, donde la clasificación es precisa.</p>

<h1 id="ejercicio-c">Ejercicio C</h1>

<p>c) Dimensionalidad: Repita el punto 7 del Práctico 1, usando k-nn. Utilice dos valores de k: el número de vecinos que se obtiene como mínimo de validación,  y 1 vecino. Genere una gráfica incluyendo también los resultados de redes, árboles y naive-Bayes con Gaussianas.</p>



<h1 id="ejercicio-d">Ejercicio D</h1>

<p>d) Opcional: Otra variante de k-nn que se suele utilizar es usar en la votación a todos los patrones que estén a una distancia menor a un dado valor D del patrón que se quiere clasificar, en lugar de usar un número fijo k. El único parámetro del  algoritmo, ahora, es la distancia máxima D, la que se optimiza utilizando un conjunto de validación. <br>
Implemente dicho algoritmo. Aplíquelo al problema de Dimensionalidad, y compare el resultado con el obtenido en el punto c). </p></div></body>
</html>