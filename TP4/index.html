<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Trabajo Práctico 4: k-vecinos</title>
<link rel="stylesheet" href="https://stackedit.io/res-min/themes/base.css" />
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body><div class="container"><h1 id="trabajo-práctico-4-k-vecinos">Trabajo Práctico 4: k-vecinos</h1>



<h2 id="ejercicios-a-y-d">Ejercicios A y D</h2>

<p>Se implementó un programa para resolver problemas de clasificación mediante el algoritmo de los <code>k</code> vecinos más cercanos (<em>k-NN</em>). El programa está realizado en C++ y puede ser compilado de dos formas distintas, dependiendo de la funcionalidad deseada por el usuario:</p>

<ul>
<li>Para clasificar observando los <code>k</code> vecinos más cercanos: <br>
<code>g++ -O2 knn.cpp -o knn.k</code></li>
<li>Para clasificar observando todos los vecinos a una distancia cuadrada <code>d</code>: <br>
<code>g++ -O2 -DUSE_DIST knn.cpp -o knn.dist</code></li>
</ul>

<p>En el primer caso, para clasificar un punto el programa busca los <code>k</code> vecinos más cercanos al punto dado según la métrica euclidiana y utiliza la clase con más representantes en el conjunto. </p>

<p>En el segundo caso, para clasificar un punto se buscan todos los vecinos a una distancia cuadrada no mayor a <code>d</code> y se utiliza la clase con más representantes en el conjunto. En caso de resultar vacío el conjunto, se utiliza la clase del vecino más cercano (similar al primer caso con <code>k = 1</code>).</p>

<p>En ambos casos, para seleccionar el valor de <code>k</code> o <code>d</code>  mediante validación se particiona equitativamente el conjunto de entrenamiento en 5, para obtener conjuntos de validación con el 20% de los datos. Luego se utiliza la mediana del error de validación en dichos conjuntos.</p>

<h1 id="ejercicio-b">Ejercicio B</h1>

<p>Se procedió a clasificar nuevamente el <em>dataset</em> de espirales anidadas anteriormente utilizado con redes neuronales. En esta ocasión se utilizó el algoritmo del vecino más cercano (<code>k = 1</code>), y se obtuvo un error de entrenamiento de 5.56% y un error de prueba de 5.8%. Este resultado es mejor que el obtenido con redes neuronales, siendo que aquellas resultaban en un error de prueba del orden de 9% con 40 neuronas en la capa oculta.</p>

<p>Al <a href="out-ejb/points-plot.pdf">graficar el conjunto de predicciones</a> se observa en general una muy buena clasificación de los puntos. Los errores se ubican sobre el área lindera entre ambos espirales. Esto se debe a que en esa zona es posible que el vecino más cercano a un punto de una espiral pertenezca a la otra, cosa que nunca ocurre en la zona interna de la espiral, donde la clasificación resulta precisa.</p>

<h1 id="ejercicio-c">Ejercicio C</h1>

<p>Para este ejercicio, se repitió lo realizado en el punto 7 del trabajo práctico 1, esta vez utilizando el clasificador <em>k-NN</em>, tanto con <code>k = 1</code> como con selección de <code>k</code> mediante validación. Se procedió luego a <a href="out-ejc/plot-all.pdf">graficar los resultados</a>.</p>

<p>En la gráfica se observa cómo los resultados son muy similares para ambos problemas, y siempre resultan mejores al utilizar <code>k</code> seleccionado por validación. En líneas generales, cuando ajustamos <code>k</code> con validación los errores se asemejan bastante a los observados mediante clasificación con redes neuronales. Si forzamos <code>k = 1</code> el error aumenta considerablemente, llegando a ser alrededor de 10 puntos porcentuales mayor en el caso de 32 dimensiones contra <code>k</code> ajustado.</p>



<h1 id="ejercicio-d-segunda-parte">Ejercicio D, segunda parte</h1>

<p>Vistos los resultados obtenidos en el ejercicio C, se implementó la votación utilizando todos los vecinos a una distancia no mayor a <code>d</code>. Este método debería permitir una mejor clasificación en problemas donde los puntos se encuentren dispersos con distinta densidad, al clasificar por cercanía en lugar de por una cantidad de vecinos, que en un área poco concentrada podrían resultar lejanos. Se procedió luego a reevaluar los datos del ejercicio anterior, y se <a href="out-ejd/plot-knn.pdf">graficó una comparación</a> de los resultados de las distintas variantes de <em>k-NN</em>.</p>

<p>En el gráfico se puede ver que los resultados entre ajustar <code>k</code> y <code>d</code> para pocas dimensiones son prácticamente los mismos, pero al aumentar el número de dimensiones el error crece más lentamente en el caso de ajustar <code>d</code>, manteniendo niveles bastante cercanos a los clasificadores de <em>Naive Bayes</em>. Esto se debe a que, para pocas dimensiones, los puntos se encuentran densamente distribuidos, por lo que ajustar <code>k</code> no resulta en el inconveniente anteriormente descrito. Recordando que la cantidad de puntos de este problema está fija, resulta luego que los puntos en los <em>datasets</em> de alta dimensionalidad se encuentran más dispersos en el espacio, y un ajuste por <code>k</code> resulta en un error mayor, siendo que puede ocurrir que se esté clasificando con vecinos que no necesariamente sean cercanos.</p></div></body>
</html>